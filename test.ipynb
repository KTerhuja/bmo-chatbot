{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI, ChatAnthropic\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader, UnstructuredPDFLoader, MathpixPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter\n",
    "# from langchain.document_loaders import UnstructuredExcelLoader\n",
    "# from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.agents.tools import Tool\n",
    "# from langchain.experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "# from langchain import OpenAI, VectorDBQA\n",
    "# from langchain.chains.router import MultiRetrievalQAChain\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"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\"\n",
    "claude_models = [\"claude-instant-1\",\"claude-2\"]\n",
    "anthropic_llm = ChatAnthropic(model=claude_models[1],temperature= 0,max_tokens_to_sample = 512)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"36ed7c12ee2344d18a3f71ddeb477ce6\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] =\"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] =\"2023-05-15\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://testavinx.openai.azure.com/\"\n",
    "\n",
    "openai_llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\",model_name=\"gpt-35-turbo\",temperature=0)\n",
    "embeddings = OpenAIEmbeddings(deployment=\"embedding1\",model=\"text-embedding-ada-002\",openai_api_base=\"https://testavinx.openai.azure.com/\",openai_api_type=\"azure\",chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(path,pages=[]):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        doc = UnstructuredPDFLoader(file_path=path)\n",
    "    else:\n",
    "        doc = DirectoryLoader(path=path,glob=\"**/*.pdf\")\n",
    "    document = doc.load()\n",
    "    if len(pages)!=0:\n",
    "        document = [document[i] for i in pages]\n",
    "    context = \"\\n\\n\".join([document[i].page_content for i in range(len(document))])\n",
    "    return context\n",
    "\n",
    "def token_len(string,model=\"text-embedding-ada-002\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "\n",
    "def create_embedding(path,embeddings):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        doc = PyPDFLoader(file_path=path)\n",
    "    else:\n",
    "        doc = DirectoryLoader(path=path,glob=\"**/*.pdf\")\n",
    "    document = doc.load()\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,length_function = token_len)\n",
    "    # text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)/\n",
    "    # text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    # document = text_splitter.split_documents(document)\n",
    "    file_name = os.path.splitext(os.path.split(path)[-1])[0]\n",
    "    if (f\"{file_name}.pkl\" not in os.listdir(\"./FAISS_VS/\")) or (f\"{file_name}.faiss\" not in os.listdir(\"./FAISS_VS/\")):\n",
    "        docsearch = FAISS.from_documents(document, embeddings)\n",
    "        docsearch.save_local(folder_path='FAISS_VS', index_name=f\"{file_name}\")\n",
    "    return document\n",
    "\n",
    "def get_relevant_context(question,docs,top_k=5):\n",
    "    context = dict()\n",
    "    for doc_name,doc_info in docs.items():\n",
    "        relevant_docs = doc_info[\"embeddings\"].similarity_search(question,k=top_k)\n",
    "        pages = [d.metadata[\"page\"] for d in relevant_docs]\n",
    "        relevant_pages = sorted(set().union(*[{p,p+1,p-1} for p in pages]).difference({-1}))\n",
    "        context[doc_name] = \"\\n\\n\".join([d.page_content for d in doc_info[\"documents\"] if d.metadata[\"page\"] in relevant_pages])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_paths = [\n",
    "    './data/Home Bancorp (HB).pdf',\n",
    "    './data/National Bank of Canada (NBC).pdf',\n",
    "    './data/Bank of Montreal (BMO).pdf',\n",
    "    './data/Basel Capital Adequacy Reporting (BCAR).pdf',\n",
    "    './data/Versa Bank (VB)'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "institute = \"Bank of Montreal (BMO)\"\n",
    "client = \"Basel Capital Adequacy Reporting (BCAR)\"\n",
    "\n",
    "docs = {\n",
    "    f\"{institute} Annual Report Document\"   : {\n",
    "        \"embeddings\":FAISS.load_local(folder_path='./FAISS_VS', embeddings=embeddings, index_name=institute),\n",
    "        \"documents\" :create_embedding(f'./data/{institute}.pdf',embeddings),\n",
    "        },\n",
    "    f\"{client} Document\"                    : {\n",
    "        \"embeddings\":FAISS.load_local(folder_path='./FAISS_VS', embeddings=embeddings, index_name=client),\n",
    "        \"documents\" :create_embedding(f'./data/{client}.pdf',embeddings),\n",
    "        } \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_answer(chat_llm,question,docs,top_k=5):\n",
    "    \n",
    "#     relevant_context = get_relevant_context(question,docs,top_k)\n",
    "    \n",
    "#     retrival_system_template = \"\"\"You are a helpful assistant, You need to extract as much text as you can which is relater or relevant to the answer of the given question from the context provided:\n",
    "    \n",
    "# Question: \"{question}\"\n",
    "\n",
    "# Context: {doc_name}\n",
    "\n",
    "# {context}\n",
    "\n",
    "# DO NOT answer the question.\n",
    "# Only extract the relevant text from the provided context.\n",
    "# DO NOT use previous knowledge only use the provided context.\n",
    "# \"\"\"\n",
    "    \n",
    "#     retrival_system_prompt = SystemMessagePromptTemplate.from_template(template=retrival_system_template)\n",
    "#     retrival_messages = [retrival_system_prompt]\n",
    "#     retrival_chat_prompt = ChatPromptTemplate.from_messages(retrival_messages)\n",
    "    \n",
    "#     summary = dict()\n",
    "#     for doc_name,doc_txt in tqdm(relevant_context.items()):\n",
    "#         retrival_llm = ChatAnthropic(model=claude_models[1],temperature= 0,max_tokens_to_sample = 512)\n",
    "#         summary[doc_name] = retrival_llm(retrival_chat_prompt.format_prompt(question=question,doc_name=doc_name,context=doc_txt).to_messages()).content\n",
    "\n",
    "#     compare_context = \"\\n\\n\".join([f\"Relevant points from {doc_name}:\\n\\n{doc_summary}\" for doc_name,doc_summary in summary.items()])\n",
    "#     print(\"\\n\\n\",compare_context,\"\\n\\n\")\n",
    "    \n",
    "#     compare_system_template = \"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute}.\n",
    "# You will be given relevant points from various documents that will help you answer the user question.\n",
    "# Below is a list of relevant points along with the name of the document from where thoes points are from.\n",
    "# Consider all the documents provided to you and answer the question by choosing all the relevant points to the question.\n",
    "# You might have to compare points from more than one document to answer the question.\n",
    "\n",
    "# {context}\"\"\"\n",
    "\n",
    "#     compare_system_prompt = SystemMessagePromptTemplate.from_template(template=compare_system_template)\n",
    "#     compare_messages = [compare_system_prompt,HumanMessage(content=question)]\n",
    "#     compare_chat_prompt = ChatPromptTemplate.from_messages(compare_messages)\n",
    "#     response = chat_llm(compare_chat_prompt.format_prompt(institute=institute,context=compare_context).to_messages()).content\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"According to the Fiscal year end mentioned in the annual report on what months should BMO file BCAR?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_system_template = \"\"\"You are a helpful assistant who has been given the question \"{question}\".\n",
    "Corresponding to previous question you have to extract the information from {institute} Annual Report provided below.\n",
    "Just extract the relevent information or text from the context below that one required to answer the above question.\n",
    "You are not question answering chatbot you just extract similar text from the context.\n",
    "\n",
    "{institute} Annual Report:\n",
    "\n",
    "{bank_context}\n",
    "\"\"\"\n",
    "    \n",
    "bank_system_prompt = SystemMessagePromptTemplate.from_template(template=bank_system_template)\n",
    "bank_chat_prompt = ChatPromptTemplate.from_messages([bank_system_prompt])\n",
    "bank_chain = LLMChain(llm=anthropic_llm, prompt=bank_chat_prompt, output_key=\"bank_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcar_system_template = \"\"\"You are a helpful assistant who has been given the question \"{question}\".\n",
    "Corresponding to previous question you have to extract the information from Basel Capital Adequacy Reporting (BCAR) Document provided below.\n",
    "Just extract the relevent information or text from the context below that one required to answer the above question.\n",
    "You are not question answering chatbot you just extract similar text from the context.\n",
    "\n",
    "Basel Capital Adequacy Reporting (BCAR) Document:\n",
    "\n",
    "{bcar_context}\n",
    "\"\"\"\n",
    "    \n",
    "bcar_system_prompt = SystemMessagePromptTemplate.from_template(template=bank_system_template)\n",
    "bcar_chat_prompt = ChatPromptTemplate.from_messages([bcar_system_prompt])\n",
    "bcar_chain = LLMChain(llm=anthropic_llm, prompt=bcar_chat_prompt, output_key=\"bcar_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_system_template = \"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute}.\n",
    "You will be given relevant points from various documents that will help you answer the user question.\n",
    "Below is a list of relevant points along with the name of the document from where thoes points are from.\n",
    "Consider all the documents provided to you and answer the question by choosing all the relevant points to the question.\n",
    "You might have to compare points from more than one document to answer the question.\n",
    "\n",
    "{institute} Annual Report:\n",
    "{bank_summary}\n",
    "\n",
    "Basel Capital Adequacy Reporting (BCAR) Document:\n",
    "{bcar_summary}\n",
    "\"\"\"\n",
    "\n",
    "compare_system_prompt = SystemMessagePromptTemplate.from_template(template=compare_system_template)\n",
    "compare_messages = [compare_system_prompt,HumanMessage(content=question)]\n",
    "compare_chat_prompt = ChatPromptTemplate.from_messages(compare_messages)\n",
    "compare_chain = LLMChain(llm=anthropic_llm, prompt=compare_chat_prompt, output_key=\"final_answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[bank_chain,bcar_chain,compare_chain],\n",
    "    input_variables=[\"bank_context\",\"bcar_context\",\"institute\",\"question\"],\n",
    "    output_variables=[\"bank_summary\",\"bcar_summary\",\"final_answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_context, bcar_context = get_relevant_context(question,docs).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = overall_chain({\"question\":question,\"institute\":institute,\"bank_context\":bank_context,\"bcar_context\":bcar_context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the information provided in the Bank of Montreal (BMO) Annual Report and Basel Capital Adequacy Reporting (BCAR) Document, the relevant points are:\n",
      "\n",
      "- The annual report states that BMO's fiscal year end is October 31.\n",
      "\n",
      "- The annual report mentions that BMO is required to meet the minimum TLAC Ratio and TLAC Leverage Ratio effective November 1, 2021, as calculated under OSFI's TLAC Guideline. \n",
      "\n",
      "- OSFI's TLAC Guideline states that D-SIBs like BMO must file BCAR on a quarterly basis.\n",
      "\n",
      "- Therefore, based on a fiscal year ending October 31, BMO should file BCAR on the following months:\n",
      "\n",
      "1) January 31 (for the quarter ending January 31)\n",
      "2) April 30 (for the quarter ending April 30)  \n",
      "3) July 31 (for the quarter ending July 31)\n",
      "4) October 31 (for the quarter ending October 31)\n",
      "\n",
      "In summary, according to its fiscal year end of October 31 mentioned in the annual report, BMO should file BCAR on January 31, April 30, July 31, and October 31 each year.\n"
     ]
    }
   ],
   "source": [
    "print(response['final_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit  as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
